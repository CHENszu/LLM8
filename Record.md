# 2025/12/15
q1：介绍几种Attention（MHA, MQA, GQA）的数学原理和区别，以及参数量的增长情况；  
q2：分别讲一下Dense模型和MoE模型以及二者的区别；  
q3：讲一下MoE的路由机制是如何做的？  
q4：介绍RAG的发展历程，各个技术在进步的时候有什么亮点？  
q5：如果召回的答案不是想要的，该怎么处理？  
q6：讲一下BM25算法原理；  
q7：是否做过意图识别？如果要做意图识别，可以怎么实现？  
q8：微调项目是如何模型选型？  
q9：如何做微调的？直接用 PEFT 库，或者用LLama Factory有什么区别？  
q10：讲一下DPO, PPO, GRPO的原理和区别，写一下各自的损失函数。  

q11：VLLM中使用的技术是否熟悉（如Paged Attention、KV Cache）？  
q12：了解加速推理框架DeepSpeed吗？  
q13：MoE模型专家的负载不均衡问题如何解决？  
q14：如何通过修改损失函数来解决负载均衡问题？  
q15：SFT使用的数据集，使用了多少张卡？SFT训练多久？  
q16：SFT的数据集是越大越好吗？会存在scaling law吗？  
q17：SFT使用的数据可能和原始模型预训练时的数据分布有较大区别，怎么解决？  
q18：讲一下LoRA微调的原理，A、B矩阵怎么初始化的，LoRA微调设置的是多少？  
q19：讲一下什么场景下用少样本，什么场景下用RL？  
q20：为什么使用强化学习会存在训练不稳定问题？为什么业界还在用？


