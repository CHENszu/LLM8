{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69094bed",
   "metadata": {},
   "source": [
    "# 大模型量化 (LLM Quantization)\n",
    "\n",
    "本笔记旨在解释大模型量化精度。\n",
    "\n",
    "## 1. 什么是大模型量化？\n",
    "\n",
    "**简单理解**：量化就是给模型“瘦身”。\n",
    "\n",
    "在深度学习中，模型的**权重（Weights）** 和 **激活值（Activations）** 通常使用 32位浮点数 (FP32)存储。这就好比用一把精度极高的游标卡尺去测量一个物体，虽然很准，但记录的数据很长，占地方。\n",
    "\n",
    "**量化 (Quantization)** 则是将高精度的浮点数（如 FP32, FP16）转换为低精度的整数（如 INT8, INT4）。这就好比改用一把普通的米尺去测量，虽然精度丢了一点点，但记录的数据变短了，不仅节省显存，还能加快计算速度（整数运算通常比浮点运算快）。\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6762aee3",
   "metadata": {},
   "source": [
    "## 2. 符号位、指数位、尾数位\n",
    "\n",
    "要理解量化，首先得知道计算机是怎么存小数的（IEEE 754 标准）。\n",
    "\n",
    "浮点数公式：\n",
    "$$ Value = (-1)^S \\times 2^{E - Bias} \\times (1 + M) $$\n",
    "\n",
    "一个浮点数由三部分组成：\n",
    "\n",
    "1.  **符号位 (Sign Bit, S)**: 决定正负。\n",
    "    *   0 代表正数 (+)，1 代表负数 (-)。\n",
    "    *   占用 1 bit。\n",
    "2.  **指数位 (Exponent, E)**: 决定数值的大小范围（Range）。\n",
    "    *   这就好比科学计数法 $1.23 \\times 10^5$ 中的 \"5\"。\n",
    "    *   指数位越多，能表示的数越大（或越小）。\n",
    "3.  **尾数位 (Mantissa / Fraction, M)**: 决定数值的精度（Precision）。\n",
    "    *   这就好比 $1.234567$ 小数点后面的数字。\n",
    "    *   尾数位越多，数字越精确，两条刻度线之间分得越细。\n",
    "\n",
    "**实例：将十进制数 5.5 转换为单精度浮点数（FP32）格式**  \n",
    "step 1: 将5.5转换为二进制\n",
    "* 整数部分5：5/2=2余1，2/2=1余0，1/2=0余1，所以整数部分为101\n",
    "* 小数部分0.5：0.5*2=1，所以小数部分为1\n",
    "所以$5.5 = 101.1$\n",
    "\n",
    "step 2: 将二进制数规格化为科学计数法形式\n",
    "$101.1 = 1.011 \\times 2^2$\n",
    "\n",
    "step 3: 确定各部分的值\n",
    "* 符号位（S） = 0（因为5.5是正数）所以$S = 0$\n",
    "* 指数位（E） = 指数为2\n",
    "* 尾数位（M） = 尾数为011\n",
    "\n",
    "step 4: 计算偏移指数\n",
    "* 单精度浮点数的偏移指数（Bias）为127，因此偏移指数=2+127=129\n",
    "\n",
    "step5：将各部分转换为二进制\n",
    "* 符号位：0\n",
    "* 指数位：129 = 10000001\n",
    "* 尾数位（补零至23位）：01100000000000000000000\n",
    "\n",
    "step6：合并成完整的32位浮点数\n",
    "* 合并后的二进制数为：0 10000001 01100000000000000000000  \n",
    "所以$5.5$的单精度浮点数表示为：0 10000001 01100000000000000000000  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e83e97ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.5\n"
     ]
    }
   ],
   "source": [
    "import struct\n",
    "# 从二进制字符串创建 FP32 浮点数\n",
    "fp32_bits = '01000000101100000000000000000000'\n",
    "fp32_bytes = bytes(int(fp32_bits[i:i+8], 2) for i in range(0, 32, 8))\n",
    "fp32_value = struct.unpack('!f', fp32_bytes)[0]\n",
    "print(fp32_value)  # 输出: 5.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73ef192",
   "metadata": {},
   "source": [
    "* PS：由于1Byte=8bit，所以32位浮点数占用4Byte内存空间。  \n",
    "Q: 那么一个 7B（70亿）参数的大模型，如果以 FP16（float16） 精度存储，所需的内存占用是多少？  \n",
    "A: 70亿个参数，每个参数占用2Byte（16bit），所以总内存占用为14GB。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfa5ee0",
   "metadata": {},
   "source": [
    "### 常见格式对比\n",
    "\n",
    "| 格式 | 总位数 | 符号位 (S) | 指数位 (E) | 尾数位 (M) | 特点 |\n",
    "| :--- | :--- | :--- | :--- | :--- | :--- |\n",
    "| **FP32** (单精度) | 32 | 1 | 8 | 23 | 精度高，范围广，训练默认格式。 |\n",
    "| **FP16** (半精度) | 16 | 1 | 5 | 10 | 显存减半，但指数位少，容易溢出 (Overflow)。 |\n",
    "| **BF16** (Brain Float) | 16 | 1 | **8** | 7 | Google 提出。**指数位与 FP32 相同**，不易溢出，适合混合精度训练。精度比 FP16 差。 |\n",
    "| **INT8** (整型) | 8 | 1 (隐式) | 0 | 7 | 没有指数位，只有整数。范围通常是 -128 到 127。 |\n",
    "\n",
    "**量化的本质**：就是试图用 INT8 这种“粗糙”的刻度，去拟合 FP16 这种“精细”的刻度，同时尽量保证误差在可接受范围内。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f852efe9",
   "metadata": {},
   "source": [
    "## 3. 核心量化原理\n",
    "\n",
    "最常用的是 **线性量化 (Linear Quantization)**。它假设浮点数 $X_{float}$ 和整数 $X_{int}$ 之间存在线性映射关系。\n",
    "\n",
    "### 核心公式\n",
    "\n",
    "$$ X_{int} = \\text{round}(\\frac{X_{float}}{S} + Z) $$\n",
    "$$ X_{dequant} = (X_{int} - Z) \\times S $$\n",
    "\n",
    "其中：\n",
    "- **$S$ (Scale, 缩放因子)**: 类似于步长。决定了两个整数之间代表的浮点距离。\n",
    "- **$Z$ (Zero-point, 零点)**: 浮点数的 0 对应整数的多少。用于对齐零点。\n",
    "\n",
    "### 两种主要模式\n",
    "\n",
    "1.  **对称量化 (Symmetric Quantization)**:\n",
    "    *   强制 $Z = 0$。\n",
    "    *   映射范围是对称的（如 -127 到 127）。\n",
    "    *   公式简化为：$X_{int} = \\text{round}(X_{float} / S)$。\n",
    "    *   **优点**：计算简单，速度快。\n",
    "    *   **缺点**：如果数据分布不对称（比如 ReLU 激活后全是正数），会浪费一半的整数范围。\n",
    "\n",
    "2.  **非对称量化 (Asymmetric Quantization)**:\n",
    "    *   $Z \\neq 0$。\n",
    "    *   可以精确映射数据的最小值 $min$ 到整数的最小值，最大值 $max$ 到整数的最大值。\n",
    "    *   **优点**：对非对称数据（如 Activation）更准确。\n",
    "    *   **缺点**：计算时需要加上零点，稍微多一点计算开销。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ba7ae8",
   "metadata": {},
   "source": [
    "## 4. 大模型量化方式总结\n",
    "\n",
    "根据量化发生的时机和对象，主要分为以下几类：\n",
    "\n",
    "### 按时机分\n",
    "\n",
    "1.  **PTQ (Post-Training Quantization, 训练后量化)**\n",
    "    *   模型训练完之后，直接对权重进行量化。不需要重新训练。\n",
    "    *   **代表算法**: GPTQ, AWQ, LLM.int8(), [SmoothQuant](https://arxiv.org/abs/2211.10438)。\n",
    "    *   **特点**: 速度快，资源消耗小，目前最主流。\n",
    "\n",
    "2.  **QAT (Quantization-Aware Training, 量化感知训练)**\n",
    "    *   在训练过程中就模拟量化带来的误差，让模型学会“适应”低精度。\n",
    "    *   **特点**: 效果最好，但训练成本高，通常用于从头预训练或深度微调。\n",
    "\n",
    "### 按对象分\n",
    "\n",
    "1.  **Weight-Only Quantization (仅权重量化)**\n",
    "    *   只把模型参数（Weights）量化为 INT4/INT8，计算时解压回 FP16 进行运算。\n",
    "    *   **目的**: 主要是为了**省显存**，让大模型能跑在消费级显卡上。\n",
    "    *   **例子**: QLoRA (4-bit), AWQ (4-bit)。\n",
    "\n",
    "2.  **Activation Quantization (激活值量化)**\n",
    "    *   不仅权重是整数，中间层的输入输出（Activations）也是整数。\n",
    "    *   **目的**: 使用整数矩阵乘法单元（如 Tensor Core 的 INT8 模式）来**加速计算**。\n",
    "    *   **难点**: 激活值主要存在 Outliers（离群点），难以量化（SmoothQuant 解决了这个问题）。\n",
    "\n",
    "3.  **KV Cache Quantization**\n",
    "    *   针对长文本推理，KV Cache 会占用大量显存。将其量化为 INT8/FP8。\n",
    "\n",
    "### 存储占用对比 (以 7B 模型为例)\n",
    "\n",
    "| 精度 | 每个参数占用 | 7B 模型总显存 (约) | 备注 |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **FP32** | 4 Bytes | 28 GB | 只有训练时用 |\n",
    "| **FP16/BF16** | 2 Bytes | 14 GB | 标准推理精度 |\n",
    "| **INT8** | 1 Byte | 7 GB | 几乎无损 |\n",
    "| **INT4** | 0.5 Byte | 3.5 GB | 稍微有损，最流行 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da79b50a",
   "metadata": {},
   "source": [
    "## 5. 写一个最简单的量化器\n",
    "\n",
    "下面我们用 Python 来实现一个简单的**对称 AbsMax 量化**，看看它是如何工作的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c000636b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始数据 (部分):\n",
      " tensor([[ 3.8538,  2.9746,  1.8014, -4.2110,  1.3568],\n",
      "        [-2.4691, -0.0861, -3.2093, -1.5043, -1.3732]])\n",
      "\n",
      "量化后的数据 (INT8) (部分):\n",
      " tensor([[ 106,   82,   49, -115,   37],\n",
      "        [ -68,   -2,  -88,  -41,  -38]], dtype=torch.int8)\n",
      "Scale (缩放因子): 0.036487\n",
      "\n",
      "反量化后的数据 (FP32) (部分):\n",
      " tensor([[ 3.8676,  2.9919,  1.7879, -4.1960,  1.3500],\n",
      "        [-2.4811, -0.0730, -3.2109, -1.4960, -1.3865]])\n",
      "\n",
      "平均绝对误差 (MAE): 0.008797\n",
      "\n",
      "原始大小: 100 bytes\n",
      "量化大小: 25 bytes (压缩率 4.0x)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def simple_quantize_int8(tensor):\n",
    "    \"\"\"\n",
    "    实现简单的对称 AbsMax 量化 (FP32 -> INT8)\n",
    "    公式: \n",
    "        scale = max(abs(tensor)) / 127\n",
    "        int8_val = round(tensor / scale)\n",
    "    \"\"\"\n",
    "    # 1. 找到最大绝对值\n",
    "    max_val = torch.abs(tensor).max()\n",
    "    \n",
    "    # 2. 计算缩放因子 Scale\n",
    "    # INT8 的范围是 [-128, 127]，为了对称通常使用 [-127, 127]\n",
    "    scale = max_val / 127.0\n",
    "    \n",
    "    # 3. 量化：除以 Scale 并四舍五入\n",
    "    quantized = torch.round(tensor / scale)\n",
    "    \n",
    "    # 4. 截断 (Clamp)：防止超出 [-127, 127] 范围\n",
    "    quantized = torch.clamp(quantized, -127, 127)\n",
    "    \n",
    "    # 5. 转换为 int8 类型以节省存储\n",
    "    quantized_int8 = quantized.to(torch.int8)\n",
    "    \n",
    "    return quantized_int8, scale\n",
    "\n",
    "def dequantize_int8(quantized_int8, scale):\n",
    "    \"\"\"\n",
    "    反量化 (INT8 -> FP32)\n",
    "    公式: fp32_val = int8_val * scale\n",
    "    \"\"\"\n",
    "    # 转换回 float 进行计算\n",
    "    return quantized_int8.float() * scale\n",
    "\n",
    "# --- 测试 ---\n",
    "\n",
    "# 1. 创建一个模拟权重的随机 Tensor (FP32)\n",
    "torch.manual_seed(42)\n",
    "original_weights = torch.randn(5, 5) * 2.0  # 放大一点数值\n",
    "print(\"原始数据 (部分):\\n\", original_weights[:2, :])\n",
    "\n",
    "# 2. 量化\n",
    "q_weights, scale = simple_quantize_int8(original_weights)\n",
    "print(\"\\n量化后的数据 (INT8) (部分):\\n\", q_weights[:2, :])\n",
    "print(f\"Scale (缩放因子): {scale.item():.6f}\")\n",
    "\n",
    "# 3. 反量化\n",
    "reconstructed_weights = dequantize_int8(q_weights, scale)\n",
    "print(\"\\n反量化后的数据 (FP32) (部分):\\n\", reconstructed_weights[:2, :])\n",
    "\n",
    "# 4. 计算误差 (量化损失)\n",
    "error = torch.abs(original_weights - reconstructed_weights).mean()\n",
    "print(f\"\\n平均绝对误差 (MAE): {error.item():.6f}\")\n",
    "\n",
    "# 5. 验证存储占用\n",
    "orig_mem = original_weights.element_size() * original_weights.numel()\n",
    "quant_mem = q_weights.element_size() * q_weights.numel()\n",
    "print(f\"\\n原始大小: {orig_mem} bytes\")\n",
    "print(f\"量化大小: {quant_mem} bytes (压缩率 {orig_mem/quant_mem:.1f}x)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdea5c6",
   "metadata": {},
   "source": [
    "### 结果分析\n",
    "\n",
    "运行上面的代码，你会发现：\n",
    "1.  **数据变了**：原始的 `1.9269` 可能变成了 `1.9213`。这就是精度损失。\n",
    "2.  **大小变了**：存储空间变成了原来的 1/4 (32bit -> 8bit)。\n",
    "3.  **Scale 很重要**：它连接了整数世界和浮点世界。\n",
    "\n",
    "在实际的大模型量化算法（如 AWQ）中，会比这个复杂得多，比如会按通道（Per-Channel）或按组（Per-Group）来计算 Scale，以减少误差。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm_identify",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
